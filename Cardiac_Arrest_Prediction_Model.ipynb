{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvn92IU12XU3aThkkFXu79",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LahiruRajapaksha/cardiac-arrest-prediction-using-bed-head-tickets/blob/colab-code/Cardiac_Arrest_Prediction_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount GDrive\n",
        "\n",
        "Let's mount your google drive. Let's use the data in the model section.\n",
        "\n",
        "First make a copy of the foler `CardiacPredictionResearchDataSet` using the following link\n",
        "https://drive.google.com/drive/folders/1uLwnHE2Ycuf83oEkgz0-3ZxMdgGHULTa?usp=drive_link"
      ],
      "metadata": {
        "id": "QuDO0CfD4G3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "Y6TAdLLh3kqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preparation"
      ],
      "metadata": {
        "id": "jEwjvzg6_avw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This function is responsible reshaping the data set according to the algorithm"
      ],
      "metadata": {
        "id": "XHJ7dEKRy2US"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VsMpUAH5ErI5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "# Breaking the data set as (112,52,20)'\n",
        "newPatient_list = np.array([])\n",
        "\n",
        "def dataGroupingAndPadding(patient_data,time_step,no_of_features,no_of_patients):\n",
        "    scalar = MinMaxScaler(feature_range=(0, 1))  # Scalar object is using in two places\n",
        "    zeros = np.zeros(no_of_features)\n",
        "    patient_groups = patient_data.groupby('ID')\n",
        "\n",
        "    newPatient_list = []\n",
        "    decision_tree_data = []\n",
        "    for x in range(no_of_patients):\n",
        "        patient_records = patient_groups.get_group((x + 1))\n",
        "        patient_records = np.array(patient_records)\n",
        "        labels = patient_records[:, -1]\n",
        "        decision_tree_data = np.append(decision_tree_data, patient_records[0])\n",
        "        no_of_records = patient_records.shape[0]\n",
        "\n",
        "        if (time_step - no_of_records) > 0:\n",
        "            # Create a zero-filled array with the same number of features as the existing records (excluding the last column)\n",
        "            zeros = np.zeros((time_step - no_of_records, patient_records.shape[1]-1))\n",
        "            # Create an array with the last column (label) repeated for the zero-filled records\n",
        "            labels_repeated = np.repeat(labels[-1], time_step - no_of_records)\n",
        "            labels_repeated = np.expand_dims(labels_repeated, axis=1)\n",
        "            # Create an array with the last column (label) repeated for the zero-filled records\n",
        "            padded_records = np.concatenate((np.hstack((zeros, labels_repeated)), patient_records))\n",
        "        else:\n",
        "            padded_records = patient_records\n",
        "\n",
        "        # Select the last 'time_step' records\n",
        "        padded_records = padded_records[-time_step:]\n",
        "\n",
        "        padded_records = scalar.fit_transform(padded_records.astype('float32'))\n",
        "\n",
        "        # Append the padded records for this patient to the list\n",
        "        newPatient_list.append(padded_records)\n",
        "\n",
        "    decision_tree_data = decision_tree_data.reshape(no_of_patients, no_of_features)\n",
        "    newPatient_list = np.array(newPatient_list)\n",
        "    return newPatient_list, decision_tree_data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Synthetic Data Vault"
      ],
      "metadata": {
        "id": "7eX86yiV_gUZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You need to install the Synthetic Data Vault (SDV) before use the functionalites of it"
      ],
      "metadata": {
        "id": "145oMYdW0J2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install sdv"
      ],
      "metadata": {
        "id": "RB74I3pHhCJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's import the data set separately. In here I only demonstrate using the `ExperimentNo1Data` data set. When you need to play with the rest please umcomment the code and do changes accordingly\n",
        "\n",
        "You can use the following format to import the dataframes\n",
        "\n",
        "`url = 'https://drive.google.com/uc?id={}'.format('file_id')`\n",
        "\n",
        "replace the `file_id` with the following ids when you need to import the relevant datasets\n",
        "\n",
        "Experiment data (Generated by SDV)\n",
        "\n",
        "|File Name|File ID|\n",
        "| :------------: | :------------: |\n",
        "|ExperimentNo1Data|1ARsDoVWyn7UcSz4wjVSESTpmPsbmXL8K|\n",
        "|ExperimentNo2Data|1l3jFQafg7TlCXIRLCuYfi2xHdYrIPHns|\n",
        "|ExperimentNo3Data|1EKQvHB4cQADVBXur3y1D5OpRWIilBmXc|\n",
        "|ExperimentNo4Data|1QlbQ7weC6S_IE97eyxDiyFGQNjppFLub|\n",
        "\n",
        "Initial Dataset\n",
        "\n",
        "|File Name|File ID|\n",
        "| :------------: | :------------: |\n",
        "|TrainDead|1kLA3rKJrFZbo-BX6AQeln9xA5k_PTkHv|\n",
        "|TrainSurvived|1sKTO3Irg38Z5MBc14Hl3MxFboFGoI5V0|\n",
        "|Test10%|1O6DvmTTYsLtm2CTqa-bSSOLyfc0C_MX8|\n",
        "|Validate10%|1DwCOO4TqQtTvAF3aZXDNfvcnEkZ2ohwa|\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6Q81cWMF6l_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When ever you need to generate new synthetic data you can run the following code. The data records for survived and dead patients needs to be generate separately.\n",
        "\n",
        "Note: If you encounter a versionConflict error in the terminal please try to restart the runtime and run the code segment again"
      ],
      "metadata": {
        "id": "V5C2YcAiov41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sdv.metadata import SingleTableMetadata\n",
        "from sdv.sequential import PARSynthesizer\n",
        "\n",
        "# Importing the patient records. Use the relevant id for import the dataset\n",
        "#This id is related to ExperimentNo1Data\n",
        "url = 'https://drive.google.com/uc?id={}'.format('1ARsDoVWyn7UcSz4wjVSESTpmPsbmXL8K')\n",
        "patient_record = pd.read_csv(url)\n",
        "# patient_record = pd.read_csv('/content/sample_data/InitialTrainData/TrainSurvived.csv')\n",
        "\n",
        "metadata = SingleTableMetadata()\n",
        "metadata.detect_from_dataframe(data=patient_record)\n",
        "metadata.update_column(\n",
        "    column_name='ID',\n",
        "    sdtype='id')\n",
        "metadata.set_sequence_key(column_name='ID')\n",
        "\n",
        "synthesizer = PARSynthesizer(\n",
        "    metadata,\n",
        "    context_columns=['Age','Gender','Alcoholic','Smoke','FHCD','TriageScore','Outcome'],\n",
        "    epochs=90,\n",
        "    verbose=True,\n",
        "    )\n",
        "\n",
        "synthesizer.fit(patient_record)\n",
        "\n",
        "# According to the experiments you need to change the number of sequences\n",
        "# to define how many sequences you want to generate\n",
        "synthetic_data = synthesizer.sample(num_sequences=74)\n"
      ],
      "metadata": {
        "id": "H0ac0AmegmU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following functions responsible for plotting the accuracy, validation accuray and loss graphs for the model"
      ],
      "metadata": {
        "id": "aY46mhLy0XJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot accuracy, validation accuracy loss graphs for models\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from six import StringIO\n",
        "from sklearn.tree import export_graphviz\n",
        "from IPython.display import Image\n",
        "import pydotplus\n",
        "\n",
        "# LSTM model\n",
        "def plot_lstm(hist):  # LSTM plotting\n",
        "    plt.figure(figsize=(13, 8))\n",
        "    plt.plot(hist.history['loss'], color='blue')\n",
        "    plt.plot(hist.history['val_loss'], color='orange')\n",
        "    plt.plot(hist.history['acc'], color='red')\n",
        "    plt.plot(hist.history['val_acc'], color='green')\n",
        "    plt.title('model loss during training')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['loss', 'val_loss', 'acc', ' val_acc'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Decision tree plotting\n",
        "def plot_decision_tree():\n",
        "    dot_data = StringIO()\n",
        "    export_graphviz(clf, out_file=dot_data,\n",
        "                    filled=True, rounded=True,\n",
        "                    special_characters=True, feature_names=feature_cols, class_names=['0', '1'])\n",
        "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
        "    graph.write_png('cardiacPatient.png')\n",
        "    Image(graph.create_png())\n",
        "\n"
      ],
      "metadata": {
        "id": "SQ21zRKJrj1P"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "Zj4-I2ng_5Qj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code for the Model"
      ],
      "metadata": {
        "id": "X6ViT0Xp0ih0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import metrics\n",
        "import argparse\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, f1_score, precision_recall_curve\n",
        "\n",
        "epocs = 100\n",
        "learningRate = 0.01\n",
        "batchSize = 10\n",
        "lstmNodes = 2\n",
        "\n",
        " ## Dataset can be found in https://zenodo.org/record/7603772\n",
        " ## Experiment data set. Import each experiment data set separately\n",
        "\n",
        "#This id is related to ExperimentNo1Data\n",
        "train_url = 'https://drive.google.com/uc?id={}'.format('1ARsDoVWyn7UcSz4wjVSESTpmPsbmXL8K')\n",
        "test_url = 'https://drive.google.com/uc?id={}'.format('1O6DvmTTYsLtm2CTqa-bSSOLyfc0C_MX8')\n",
        "validate_url = 'https://drive.google.com/uc?id={}'.format('1DwCOO4TqQtTvAF3aZXDNfvcnEkZ2ohwa')\n",
        "\n",
        "patients_training_data = pd.read_csv(train_url)\n",
        "patients_test_data = pd.read_csv(test_url)\n",
        "patients_validation_data = pd.read_csv(validate_url)\n",
        "\n",
        "time_step = int(patients_training_data.groupby('ID').count().mean()[0])\n",
        "no_of_patients = patients_training_data['ID'].max()\n",
        "no_of_test_patients = patients_test_data['ID'].max()\n",
        "no_of_validation_patients = patients_validation_data['ID'].max()\n",
        "\n",
        "training_LSTM_data, training_decisionTree_data = dataGroupingAndPadding(patients_training_data,time_step,20,no_of_patients)\n",
        "testing_LSTM_data, testing_decisionTree_data = dataGroupingAndPadding(patients_test_data, time_step, 20,no_of_test_patients)\n",
        "validation_LSTM_data, validation_decisionTree_data = dataGroupingAndPadding(patients_validation_data,time_step,20,no_of_validation_patients)\n",
        "\n",
        "# Shuffle the data set in the same order\n",
        "training_LSTM_data = training_LSTM_data.tolist()\n",
        "training_decisionTree_data = training_decisionTree_data.tolist()\n",
        "shuffleDataSet = list(zip(training_LSTM_data, training_decisionTree_data))\n",
        "random.shuffle(shuffleDataSet)\n",
        "training_LSTM_data, training_decisionTree_data = zip(*shuffleDataSet)\n",
        "training_LSTM_data = np.asarray(training_LSTM_data)\n",
        "training_decisionTree_data = np.asarray(training_decisionTree_data)\n",
        "\n",
        "# Prepare data set for LSTM model\n",
        "lstm_patient_training_data = training_LSTM_data[:, :, 1:7]\n",
        "lstm_patient_training_data_label = training_LSTM_data[:, :, -1].reshape(no_of_patients, time_step, 1)\n",
        "\n",
        "# Test data set for LSTM model\n",
        "lstm_patient_test_data = testing_LSTM_data[:, :, 1:7]\n",
        "lstm_patient_test_data_label =testing_LSTM_data[:, :, -1].reshape(no_of_test_patients, time_step, 1)\n",
        "\n",
        "# Validation data set for LSTM model\n",
        "lstm_validation_data =validation_LSTM_data[:, :, 1:7]\n",
        "lstm_validation_data_label =validation_LSTM_data[:, :, -1].reshape(no_of_validation_patients,time_step,1)\n",
        "\n",
        "\n",
        "# Prepare data set for Decision Tree\n",
        "d_tree_training_data = training_decisionTree_data[:, 7:19]\n",
        "d_tree_training_data_label = np.reshape(training_decisionTree_data[:, -1], (no_of_patients, 1))\n",
        "\n",
        "# Defining the model\n",
        "model = Sequential()\n",
        "model.add(LSTM(lstmNodes, return_sequences=True, input_shape=(time_step, 6)))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(Adam(learning_rate=learningRate), loss='binary_crossentropy', metrics=['acc'])  # lr0.005 0r 0.001\n",
        "model.summary()\n",
        "\n",
        "# Fit the model LSTM\n",
        "history = model.fit(lstm_patient_training_data, lstm_patient_training_data_label,\n",
        "                    shuffle=False, validation_data=(lstm_validation_data,lstm_validation_data_label), batch_size=batchSize, epochs=epocs)\n",
        "\n",
        "acc = np.round(history.history['acc'][epocs-1], 2)\n",
        "val_acc = np.round(history.history['val_acc'][epocs-1], 2)\n",
        "print('###Model Summary###')\n",
        "print(f'Accuracy {np.round(acc, 3)} ValidationAccuracy {np.round(val_acc, 3)}')\n",
        "modelName = f'acc:{acc} val_acc:{val_acc} epocs:{epocs} nodes:{lstmNodes}'\n",
        "\n",
        "\n",
        "# Getting the latent vector space from lstm layers\n",
        "outputs = []\n",
        "for layer in model.layers:\n",
        "    keras_function = K.function([model.input], [layer.output])\n",
        "    outputs.append((keras_function([lstm_patient_training_data])))\n",
        "latentVectorSpace = (outputs[0][0])[:, (time_step-1), -1]\n",
        "latentVectorSpace = np.reshape(latentVectorSpace, (no_of_patients, 1))\n",
        "\n",
        "#Append latent vector with other data\n",
        "d_tree_training_data_combined = np.append(d_tree_training_data, latentVectorSpace, axis=1)\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(lstm_patient_test_data, lstm_patient_test_data_label)\n",
        "\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Feature selection for decision tree model\n",
        "feature_cols = ['Age', 'Gender', 'GCS', 'Na', 'K', 'Cl', 'Urea', 'Creatinine', 'Alcoholic', 'Smoke',\n",
        "                'FHCD', 'TriageScore', 'LSTM']\n",
        "\n",
        "# Split dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(d_tree_training_data_combined, d_tree_training_data_label,\n",
        "                                                    test_size=0.3, random_state=1)  # 70% training and 30% test\n",
        "clf = DecisionTreeClassifier()\n",
        "clf = clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Plotting accuracy metrices\n",
        "plot_lstm(history)\n",
        "plot_decision_tree()\n",
        "\n",
        "## Prediction\n",
        "lstm_probabilities_prediction_ = model.predict(lstm_patient_training_data)\n",
        "lstm_proba_prediction_label = np.reshape(lstm_probabilities_prediction_, (no_of_patients * time_step))\n",
        "lstm_true_label = np.reshape(lstm_patient_training_data_label, (no_of_patients * time_step))\n",
        "lstm_class_label_prediction = np.reshape((model.predict(lstm_patient_training_data) > 0.5).astype(\"int32\"), (no_of_patients * time_step))\n",
        "\n",
        "print(\"==========LSTM Confusion Matrix==========\")\n",
        "print(metrics.confusion_matrix(lstm_true_label, lstm_class_label_prediction))\n",
        "print(lstm_class_label_prediction)\n",
        "lstm_falsePositive, lstm_truPositive, thresholds = roc_curve(lstm_true_label, lstm_proba_prediction_label)\n",
        "lstm_precision, lstm_recall, lstm_recall_thresholds = precision_recall_curve(lstm_true_label, lstm_proba_prediction_label)\n",
        "try:\n",
        "    lstm_auc_score = roc_auc_score(lstm_true_label, lstm_proba_prediction_label)\n",
        "    print(\"AUC Score\",lstm_auc_score)\n",
        "except ValueError:\n",
        "    pass\n",
        "lstm_f1_score = f1_score(lstm_true_label, lstm_class_label_prediction)\n",
        "print(\"F1 Score\",lstm_f1_score)\n",
        "\n",
        "# Decision tree model evaluation (Finaldecision tree model that combines LSTM data)\n",
        "print(\"==========Decision Tree Confusion Matrix======\")\n",
        "print(metrics.confusion_matrix(y_test, y_pred))\n",
        "d_tree_false_positive, d_tree_true_positive, d_tree_treshold = roc_curve(y_test, y_pred)\n",
        "d_tree_precision, d_tree_recall, d_tree_recall_threshold = precision_recall_curve(y_test, y_pred)\n",
        "d_tree_auc_score = roc_auc_score(y_test, y_pred)\n",
        "d_tree_f1_score = f1_score(y_test,y_pred)\n",
        "print(\"AUC Score\",d_tree_auc_score)\n",
        "print(\"F1 Score\",d_tree_f1_score)"
      ],
      "metadata": {
        "id": "Sd8FkYz3pGq4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}